

    def combined_action(self, env,current_episode):
        """
        Decide the action for the AI (Player 2) using logical rules, MCTS, and DQN.
        """
        current_player = env.current_player
        device = self.policy_net.device  # Access the device from the policy_net
        state_tensor = torch.tensor(env.board, dtype=torch.float32).unsqueeze(0).to(device)
        q_values = self.policy_net(state_tensor).detach().cpu().numpy().squeeze()
        q_values =normalize_q_values(q_values)
        valid_actions = env.get_valid_actions()
        valid_q_values = {action: q_values[action] for action in valid_actions}
        
        formatted_q_values = {action: f"{q_value:.3f}" for action, q_value in valid_q_values.items()}
        logging.debug(f"Q-values: {formatted_q_values}")
        # Select the action with the highest Q-value
        action = max(valid_q_values, key=lambda a: valid_q_values[a])
        max_q_value = valid_q_values[action]
        
        if max_q_value < 0.5 and EPSILON > 0.1:
            # Use logic-based or MCTS if Q-values are too low or it's the beginning of training
            if current_episode > 10000:  # Set a cap for the number of simulations
                set_simulations = 10000
            else:
                set_simulations = current_episode

            # Perform Monte Carlo Tree Search
            mcts = self.monte_carlo_tree_search(env, num_simulations=set_simulations)
            if mcts is not None:
                logging.debug(f"Player {current_player}: Using MCTS for action: {mcts}")
                return mcts
        # Log and return the action chosen by Q-value
        logging.debug(f"Player {current_player}: Selected Column {action}, Q-value: {max_q_value:.3f}")
        return action
